{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1: Get the data loaded. For this example, we are using 1000 samples from the twitter data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "from io import StringIO  # Python3\n",
    "import json\n",
    "import sys\n",
    " \n",
    "#Data pull gives the data as stdout, so running the command and extracting from there \n",
    " \n",
    "old_stdout = sys.stdout\n",
    "result = StringIO()\n",
    "sys.stdout = result\n",
    " \n",
    "%%run ../../sample_data/preprocessing.py #run preprocessing script \n",
    "%cd ../.. #cd-ing to the main dir to access sample_data module  \n",
    "from sample_data import preprocessing, data_pull\n",
    "\n",
    "data_pull.data_puller('Twitter', 1000, 1, 'username')\n",
    " \n",
    "sys.stdout = old_stdout\n",
    "\n",
    "result_string = result.getvalue()\n",
    "result = result_string.rstrip()\n",
    "\n",
    "data = json.loads(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 2: Now, we create a Mistral large model that categorizes the sentiment of a post's text component. \n",
    "The output will function as our training set for the new model and is stored in json form, with some human-evaluated examples included for the model benefit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "load_dotenv()\n",
    "\n",
    "# Replace with your own Mistral API Key\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mistralai.client import MistralClient\n",
    "from mistralai.models.chat_completion import ChatMessage\n",
    "\n",
    "model = \"mistral-large-latest\"\n",
    "\n",
    "\n",
    "client = MistralClient(api_key=api_key)\n",
    "\n",
    "\n",
    "#TODO: not sure if we should be ranking here? feels like the same problem as before \n",
    "prompt = f\"\"\"You are a helpful assistant that processes text and returns results in JSON format. \n",
    "Assign each piece of item with one of the following categorizations, based off of the sentiment of their text: very negative, negative, neutral, positive, very positive.\n",
    "For each item, give output as a JSON array with the item index coming first and the categorization coming second, \n",
    "like so: [{\"item_idx\": int, \"sentiment\": str}].\n",
    "\"\"\"\n",
    "\n",
    "post_text = \"\"\n",
    "for i, item in enumerate(data):\n",
    "        post_text += f\"ITEM: {i}:\\n{item['text']}\\n\\n\"\n",
    "\n",
    "#something still feels off here--not sure this is right \n",
    "\n",
    "#this func is based off of implementation found here: \n",
    "#https://www.datacamp.com/tutorial/guide-to-working-with-the-mistral-large-model\n",
    "        \n",
    "def chat_mistral(prompt: str):\n",
    "   messages = [  \n",
    "       ChatMessage(role=\"user\", content = prompt), \n",
    "       ChatMessage(role=\"user\", content = \"ITEM 0:\\nI love you.\\n\\nITEM 1:\\nI hate you.\\n\\nITEM 2:\\nI am indifferent to you.\\nITEM 3:\\nI like soup\\n\\n\"),\n",
    "       ChatMessage(role=\"assistant\", content = '[ {\"item_idx\": 0, \"sentiment\": \"very positive\"}, {\"item_idx\": 3, \"sentiment\": \"positive\"}, {\"item_idx\": 2, \"sentiment\": \"neutral\"}, {\"item_idx\": 1, \"sentiment\": \"negative\"} ]')\n",
    "       ChatMessage(role=\"user\", content = post_text)\n",
    "   ]\n",
    "\n",
    "   # No streaming\n",
    "   chat_response = client.chat(\n",
    "       model=model,\n",
    "       messages=messages,\n",
    "   )\n",
    "\n",
    "   return chat_response.choices[0].message.content.strip()\n",
    "\n",
    "mistral_data = chat_mistral(prompt) #not isolating ranking here because we primarily want to check if the sentiment \n",
    "#analysis is comparable, ranking will be done the same way across 5 categories regardless \n",
    "\n",
    "\n",
    "#json_results = response.choices[0].message.content.strip()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: With the output of our initial results, let's train a new mistral model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "\n",
    "# Sample data for training Mistral model\n",
    "# Assuming you have human-labeled examples in the format (prompt, response, sentiment_label)\n",
    "training_data = [\n",
    "    (\"I love you.\", \"very positive\", 1),  # Positive example\n",
    "    (\"I hate you.\", \"very negative\", 0),  # Negative example\n",
    "    (\"I am indifferent to you.\", \"neutral\", 2),  # Neutral example\n",
    "    (\"I like soup.\", \"positive\", 1)  # Positive example\n",
    "]\n",
    "\n",
    "# Tokenize the prompts and responses\n",
    "prompts, responses, labels = zip(*training_data)\n",
    "tokenizer = tf.keras.preprocessing.text.Tokenizer()\n",
    "tokenizer.fit_on_texts(prompts + responses)\n",
    "prompt_sequences = tokenizer.texts_to_sequences(prompts)\n",
    "response_sequences = tokenizer.texts_to_sequences(responses)\n",
    "max_prompt_length = max(len(seq) for seq in prompt_sequences)\n",
    "max_response_length = max(len(seq) for seq in response_sequences)\n",
    "padded_prompt_sequences = pad_sequences(prompt_sequences, maxlen=max_prompt_length)\n",
    "padded_response_sequences = pad_sequences(response_sequences, maxlen=max_response_length)\n",
    "labels = np.array(labels)\n",
    "\n",
    "model = Sequential([\n",
    "    Embedding(input_dim=len(tokenizer.word_index)+1, output_dim=100, input_length=max_prompt_length),\n",
    "    LSTM(128),\n",
    "    Dense(5, activation='softmax')  # 5 is based on the 5 possible sentiment classifications \n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the Mistral model\n",
    "model.fit([padded_prompt_sequences, padded_response_sequences], labels, epochs=10, batch_size=1)\n",
    "\n",
    "# Save the trained Mistral model\n",
    "model.save(\"mistral_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: But, we can do better. Let's use a similar structure with our simpler ChatGPT model to create a dataset that can finetune the Mistral model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flask import Flask, jsonify, request\n",
    "from flask_cors import CORS\n",
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(api_key=os.environ.get(\"OPENAI_API_KEY\"))\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "        model=\"gpt-3.5-turbo\",\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"system\",\n",
    "                \"content\": 'You are a helpful assistant that processes text and returns results in JSON format. Reorder the items you are given in terms of their positivity, with the most positive item first, and include your reasoning. Give me a JSON array in the following format: [ {\"item_idx\": int, \"reason\": str} ]',\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": \"ITEM 0:\\nI love you.\\n\\nITEM 1:\\nI hate you.\\n\\nITEM 2:\\nI am indifferent to you.\\nITEM 3:\\nI like soup\\n\\n\",\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"content\": '[ {\"item_idx\": 0, \"reason\": \"The statement is very positive.\"}, {\"item_idx\": 3, \"reason\": \"The statement is somewhat positive.\"}, {\"item_idx\": 2, \"reason\": \"The statement is neutral.\"}, {\"item_idx\": 1, \"reason\": \"The statement is negative.\"} ]',\n",
    "            },\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt,\n",
    "            },\n",
    "        ],\n",
    "    )\n",
    "\n",
    "chatgpt_data = response.choices[0].message.content.strip() #same situation here, don't need the ranking immediately \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
